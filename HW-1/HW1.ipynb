{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"colab":{"name":"HW1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github","colab_type":"text"},"source":["<a href=\"https://colab.research.google.com/github/will-bell/EC-CS523-Deep-Learning/blob/master/HW-1/HW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7IbwLOEjT4I4"},"source":["<h4 align=\"right\">by Ruizhao Zhu. Modified from the version by Ali Siahkamari<br> </h4>\n","\n","# Problem Set 1 \n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"jpjvDX8GT4I_"},"source":["## Preamble\n","\n","To run and solve this assignment, you need an interface to edit and run ipython notebooks (`.ipynb` files). The easiest way to complete this assignment is to use Google Colab. You can just copy the assignment notebook to your google drive and open it, edit it and run it on Google Colab. All libraries you need are pre-installed on Colab."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LnuKbS2p3cRr"},"source":["### Local installation\n","The alternative is to have a local installation, although we do not recommend it. If you are working on Google Colab, feel free to skip to the next section \"More instructions\". We recommend using virtual environments for all your installations. Following is one way to set up a working environment on your local machine for this assignment, using [Anaconda](https://www.anaconda.com/distribution/): \n","\n","- Download and install Anaconda following the instructions [here](https://docs.anaconda.com/anaconda/install/)\n","- Create a conda environment using `conda create --name dl_env python=3` (You can change the name of the environment instead of calling it `dl_env`)\n","- Now activate the environment using : `conda activate dl_env`\n","- Install jupyter lab, which is the [jupyter project's](https://jupyter.org/index.html) latest notebook interface : `pip install jupyterlab`. You can also use the classic jupyter notebooks and there isn't any difference except the interface.\n","- Install other necessary libraries. For this assignment you need `numpy`, `scipy` , [`pytorch`](https://pytorch.org/get-started/locally/) and `matplotlib`, all of which can be installed using : `pip install <lib_name>`. Doing this in the environment, would install these libraries for `dl_env`. You can also use `conda install`.\n","- Now download the assignment notebook in a local directory and launching `jupyter lab` in the same directory should open a jupyter lab session in your default browser, where you can open and edit the ipython notebook.\n","- For deactivating the environment when you are done with it, use : `conda deactivate`.\n","\n","For users running a Jupyter server on a remote machine :\n","- Launch Jupyter lab on the remote server (in the directory with the homework ipynb file) using : `jupyter lab --no-browser --ip=0.0.0.0`\n","- To access the jupyter lab interface on your local browser, you need to set up ssh port forwarding. This can be done by running : `ssh -N -f -L localhost:8888:localhost:8888 <remoteuser>@<remotehost>`. You can now open `localhost:8888` on your local browser to access jupyter lab. This assumes you are running jupyter lab on its default port 8888 on the server.\n","- Check \"Making life easy\" section at the end of [this post](https://ljvmiranda921.github.io/notebook/2018/01/31/running-a-jupyter-notebook/) to find how to add functions to your bash run config to do this more easily each time. The post mentions functions for jupyter notebook, but just replace those with jupyter lab if you are using that interface.\n","\n","The above instructions specify one way of working on the assignment. You can use other virtual environments/ipython notebook interfaces etc. (**not recommended**)."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pM-2fG3N4LaC"},"source":["### More instructions\n","\n","If you are new to Python or its scientific library, Numpy, there are some nice tutorials [here](https://www.learnpython.org/) and [here](http://www.scipy-lectures.org/).\n","\n","In an ipython notebook, to run code in a cell or to render [Markdown](https://en.wikipedia.org/wiki/Markdown)+[LaTeX](https://en.wikipedia.org/wiki/LaTeX) press `Ctrl+Enter` or `[>|]`(like \"play\") button above. To edit any code or text cell (double) click on its content. To change cell type, choose \"Markdown\" or \"Code\" in the drop-down menu above.\n","\n","To enter your solutions for the written questions, put down your derivations into the corresponding cells below using LaTeX. Show all steps when proving statements. If you are not familiar with LaTeX, you should look at some tutorials and at the examples listed below between \\$..\\$. We will not accept handwritten solutions.\n","\n","Put your solutions into boxes marked with **`[double click here to add a solution]`** and press Ctrl+Enter to render text. (Double) click on a cell to edit or to see its source code. You can add cells via **`+`** sign at the top left corner.\n","\n","**Submission instructions:** please upload your completed solution file (having run all code cells and rendered all markdown/Latex) to the Google Form posted on Piazza by the due date (see Schedule for due dates and late policy).  \n","\n","Note: `Vector` stands for `column vector` below. "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rZ5Ph9quT4JF"},"source":["### Problem 1: Review of Probability and Statistics [20 points]\n","\n","**Q1.1**: $X$ and $Y$ are random variables. The expectaion of $X$ is  $E(X)$ and the variance of $X$ is $Var(X)$.  The expectaion of $Y$ is  $E(Y)$ and the variance of $Y$ is $Var(Y)$. Prove the following:\n","\n","(a) $E(X+Y) = E(X)+E(Y)$\n","\n","(b) $Var(X+Y) = Var(X) + Var(Y) + 2Cov(X,Y)$, where $Cov(X,Y)$ is covariance of X and Y.\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"chIChbwfIv9f"},"source":["**`[double click here to add a solution]`**"]},{"cell_type":"markdown","metadata":{"id":"NdcYdYVspi1X","colab_type":"text"},"source":["This is a test2 for the commit"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ERu4tkxOIv9i"},"source":["**Q1.2** Suppose $X_1,\\dots,X_N$ have mean $\\mu$ and variance $\\sigma^2$ and are independent. Let $A= (X_1 + .. + X_N)/N$ be the empirical mean.  Show that with probability at least 0.99, $|A-\\mu| < 10\\sigma / \\sqrt{N}$"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"p_ZKL_5XIv9k"},"source":["**`[double click here to add a solution]`**"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zdDnhTezIv9p"},"source":["**Q1.3**:Assume that we are given $n$ iid samples $(x_1, ..., x_n)$ from each $P(x \\ | \\ \\theta)$ given below. Compute the maximum likelihood estimates (MLEs) for the parameter $\\theta$ in (a)(b) and $\\alpha,  \\beta$ in (c) of the given distributions. \n","\n","\n","(a) $P(x \\ | \\ \\theta) = \\frac{2 x}{\\theta^2} e^{\\frac{-x^2}{\\theta^2}}$ for $x \\geq 0$\n","\n","(b) $P(x \\ | \\ \\theta) = \\frac{1}{1-\\theta}$ for $ \\theta \\leq x \\leq 1$ \n","\n","(c): $P(x \\ | \\ \\alpha, \\beta) \\propto  \\frac{1}{x^{\\alpha+1}}$ for $ x \\geq \\beta$ \n","\n","(If $x$ is not in the support of the distribution defined by inequalities, the probability of it is $0$.)\n","\n","**Hint for Q1.3(c):** first calculate the normalizing constant by making sure the density integrates to 1."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"B1BoB_qyT4JJ"},"source":["**`[double click here to add a solution]`**"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bI8M-uRuIv9t"},"source":["**Q1.4** (Naive Bayes Maximum Likelihood) Consider binary dataset $S$ with observations in the form $\\left.\\left\\{\\left(x_{j}^{1}, \\ldots, x_{j}^{n}\\right), y_{j}\\right)\\right\\}$. Each data $\\mathbf{x_j}$ is an n-dimensional vector. Define $c(y)$ as a function that counts the number of observations such that the label is $y$. $S$ is the set of all the training data.\n","$$\n","c(y)=\\sum_{\\left(x_{j}, y_{j}\\right) \\in S}\\left[y_{j}=y\\right]\n","$$\n","Define $c(i, y)$ as a function that counts the number of observations such that the label is $y$ and $x^{i}=1$\n","$$\n","c(i, y)=\\sum_{\\left(x_{j}, y_{j}\\right) \\in S}\\left[y_{j}=y, x_{j}^{i}=1\\right]\n","$$\n","Define $b$ as $P(Y=1),$ and $b^{i y}$ as $P\\left(X^{i}=1 \\mid Y=y\\right) .$ Prove that the following estimators are MLE for these parameters:\n","$$\n","\\widehat{b}_{M L E}=\\frac{c(1)}{|S|} \\quad \\text { and } \\quad \\widehat{b^{i y}}_{M L E}=\\frac{c(i, y)}{c(y)}\n","$$"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-L86HfvtIv9u"},"source":["**`[double click here to add a solution]`**"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Evp1_DmHT4JP"},"source":["### Problem 2: Matrix Derivatives [15 points]\n","\n","**Multivariate Gaussian**\n","\n","Assume that our data is distributed according to a $\\underline d$ dimensional [multivariate Gaussian](https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Likelihood_function) with $\\bar \\mu$ mean and $\\Sigma$ covariance matrix: $$(\\mathbf x_1, \\dots, \\mathbf x_n) \\sim \\mathcal N(\\bar \\mu, \\Sigma).$$ \n","\n","**Q2.1:** \n","Using rules of [matrix derivatives](http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf) Equation [57, 59], derive $\\frac{\\partial \\mathcal L(\\theta)}{\\partial \\Sigma}$ in matrix form and set it to zero to find $\\Sigma_{ML}$ . \n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"v1hhI03PT4JU"},"source":["**`[double click here to add a solution]`**"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2FWewuNzT4Jc"},"source":["**Q2.2: Multi-target Linear Regression**\n","- we have $X \\in \\mathbf R^{n \\times d}$ is a constant data matrix\n","- $\\theta$ is a $d \\times m$-dimensional **weight matrix**\n","- $\\varepsilon_{ij} \\sim \\mathcal N(0, \\sigma_\\epsilon)$ is a normal noise ($i \\in [0; n], j \\in [0;m]$)\n","- and we observe a matrix $Y = X\\theta + \\varepsilon \\in \\mathbf R^{n \\times m}$\n","\n","$$\\varepsilon = Y - X\\theta \\sim \\mathcal N_n(0, \\sigma_\\epsilon I)$$\n","\n","$$\\mathcal L(\\theta) = \\log P(Y \\ | \\ X,\\theta) = \\log \\mathcal N_n(Y - X\\theta \\ | \\ 0, \\sigma_\\epsilon I)$$\n","\n","$$\\theta_{MLE} = \\arg \\max_{\\theta} \\mathcal L(\\theta) = \\arg \\min_{\\theta} \\text{loss}(\\theta) = \\arg \\min_{\\theta} \\big( ||Y-X\\theta||^2_F \\big)$$\n","\n","Assume $\\theta \\sim \\mathcal N(0, \\sigma_\\theta I)$, which essentially means that \"weight vector components should not be too far from zero\".\n","Where $I$ stands for an identity matrix. **Using rules of [matrix derivatives](http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf)** Equation [137, 132], show for an MLE loss:  $\\text{loss}(\\theta) = ||Y-X\\theta||^2_F$ that:\n","\n","**Q2.2.1:** derive $\\frac{\\partial\\text{loss}(\\theta)}{\\partial \\theta} = -2X^T(Y-X\\theta)$\n","\n","**Q2.2.2:** derive $\\theta_{MLE} = (X^T X)^{-1} X^T Y$\n","\n","**Hint:** In our case [see Matrix Cookbook, eq. 137], $g(U) = ||U||^2_F$ - squared Frobenius norm and $U(\\theta) = f(\\theta) = Y - \\theta X$ - linear mapping.\n","\n","**Note:** That is a multi-target problem, therfore $\\theta$ is a matrix, so you have to take the derivative wrt matrix."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"61RyTcnxIv93"},"source":["**Q2.2.3**\n","To make use of prior information,  one might want to find [maximum a posteriori estimation](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation) (MAP). In this problem, the MAP is defined as  $\\theta_{MAP} = \\arg\\max_\\theta L_{MAP}(\\theta) = \\arg\\max_\\theta P(\\theta | X, Y)$. The priori of $\\theta$ is $P(\\theta)$. Show $\\mathcal L_{MAP}(\\theta)$ can be defined as $\\mathcal L_{MLE}(\\theta) + \\log P(\\theta)$.\n","\n","**Q2.2.4**\n","Assume the prior $\\theta \\sim \\mathcal N(0, \\sigma_\\theta I)$, which essentially means that \"weight vector components should not be too far from zero\". \n","Show MAP with gaussian prior is L2 regularization.\n","\n","**Q2.2.5**\n","Now assume that $\\theta$ follows a [Laplace distribution](https://en.wikipedia.org/wiki/Laplace_distribution) i.e: $\\theta_i \\sim Laplace(0, b),  \\forall i $\n","Show MAP with laplace prior is L1 regularization. "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xRfX3jGbIv-A"},"source":["**`[double click here to add a solution]`**\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"JN0bwdRkT4Ji"},"source":["### Problem 3: Gradient for Maximum Likelihood Estimation [20 points]\n","\n","Sometimes, we can't find MLEs in closed-form, in this case, we can use an iterative scheme to solve the resulting optimization problem $\\theta_{MLE} = \\arg\\max P(x \\ | \\ \\theta)$. First-order iterative schemes (e.g. Gradient Descent, Stochastic Gradient Descent, Nesterov Accelerated Gradient Descent) require gradient information at each visited point. Compute gradients for log-likelihood of the following model:\n","\n","- we have $X \\in \\mathbf R^{n \\times d}$ - constant data matrix, $\\mathbf x_i$ - vector corresponding to a single data point\n","- $\\mu_1$ and $\\mu_2$ are $d$-dimensional (unknown) parameters\n","\n","We know that the data comes from a mixture of two $d$-dimensional guassians with mean $\\mu_1$ and $\\mu_2$.\n","\n","Which means that\n","$$ P(\\mathbf x_i \\ | \\ \\mathbf \\mu_1 ,\\mathbf \\mu_2 , \\sigma) = \\frac{1}{2} \\frac{1}{(2\\pi \\sigma^2)^\\frac{d}{2}}\\Big(\\exp(-\\frac{||x_i -\\mu_1||_F^2}{2\\sigma^2}) + \\exp(-\\frac{||x_i -\\mu_2||_F^2}{2\\sigma^2})\\Big)$$\n","\n","where $\\sigma$ is a constant parameter.\n","\n","Denote log-likelihood as $\\mathcal L(\\mu_1,\\ \\mu_2)$ and log-likelihood for a single sample $i$ as $\\mathcal L_i = \\log P(\\mathbf x_i \\mid \\mathbf \\mu_1 ,\\mathbf \\mu_2 , \\sigma)$.\n","\n","**Q3.1**: Find $\\frac{\\partial \\mathcal L_i}{\\partial \\mu_1} $."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pQH_WXGcT4Jo"},"source":["**`[double click here to add a solution]`**"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ZEiOksrQT4Jr"},"source":["**Q3.2**: Consider $\\sigma = 1$ and $d=4$ Check the result obtained in the previous section by completing the SGD code for minimising and plotting $-\\mathcal L(\\mu_1, \\mu_2)$. Try multiple learning rates (1e-1, 1e-3, 1e-6), explain the result.\n","\n","We marked places where you are expected to add/change your own code with **`## -- ! code required`** comment."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"RDthISZbT4Ju","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def generate_data(n, d, sigma):\n","    mu1 = np.random.rand(d,1)\n","    mu2 = np.random.rand(d,1)\n","    z = np.random.binomial(1,0.5, size = [1,n])\n","    X = z*np.random.normal(loc=mu1, scale=sigma, size=[d,n]) + \\\n","    (1-z)*np.random.normal(loc=mu2, scale=sigma, size=[d,n])\n","    return X\n","\n","def loss(X, mu1, mu2, sigma):\n","    loss = 0 ## -- ! code required\n","    return loss\n","\n","def loss_grad_i(X, mu1, mu2, i, sigma):\n","    grad_loss_mu1_i= 0   ## -- ! code required               \n","    grad_loss_mu2_i= 0   ## -- ! code required\n","                  \n","    return grad_loss_mu1_i, grad_loss_mu2_i\n","\n","def sgd_plot(mu1_init, mu2_init, X, sigma, n_steps, learning_rates):\n","    for learning_rate in learning_rates:\n","        mu1 = mu1_init[:]\n","        mu2 = mu2_init[:]\n","        losses = []\n","        for step_n in range(n_steps):\n","            for i in range(X.shape[1]):\n","                grad_loss_mu1_i, grad_loss_mu2_i = loss_grad_i(X, mu1, mu2, i, sigma)\n","                ## update of mu 1 and mu 2\n","                mu1 = 0 ## -- ! code required\n","                mu2 = 0 ## -- ! code required\n","                loss_i = loss(X, mu1, mu2, sigma)\n","                losses.append(loss_i)\n","        plt.plot(losses)\n","        plt.title('SGD, learning rate = '+ str(learning_rate))\n","        plt.show()\n","    \n","def main(n, d, sigma, n_steps):\n","    np.random.seed(0)\n","    X = generate_data(n, d, sigma)\n","    mu1_init = np.random.rand(d,1)\n","    mu2_init = np.random.rand(d,1)\n","    sgd_plot(mu1_init, mu2_init, X, sigma, n_steps,learning_rates = [1e-1, 1e-3, 1e-6])\n","    \n","main(100,4,1,100)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0dhGEnaCIv-N"},"source":["**`[double click here to add a Description]`**"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"JlKxC0nHIv-O"},"source":["\n","**Q3.3**: Consider $\\sigma = 1$ and $d=4$ Check the result obtained in the previous section by completing the [Adam](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam) code for minimising and plotting $-\\mathcal L(\\mu_1, \\mu_2)$. Try multiple learning rates $(1e-1, 1e-3, 1e-6)$, explain the result and compare with sgd under the same random seed.\n","\n","We marked places where you are expected to add/change your own code with **`## -- ! code required`** comment.\n","\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mlyxO9UVIv-P","colab":{}},"source":["import math\n","def Adam_plot(mu1_init, mu2_init, X, sigma, n_steps, learning_rates,m1_init,v1_init,m2_init,v2_init, beta1 = 0.9, beta2 = 0.999 ):\n","    for learning_rate in learning_rates:\n","        mu1 = mu1_init[:]\n","        mu2 = mu2_init[:]\n","        mt1 = m1_init[:]\n","        vt1 = v1_init[:]\n","        mt2 = m2_init[:]\n","        vt2 = v2_init[:]\n","        epsilon = 0.00000001\n","        losses = []\n","        for step_n in range(n_steps):\n","            for i in range(X.shape[1]):\n","                t = step_n * n_steps + i\n","                grad_loss_mu1_i, grad_loss_mu2_i = loss_grad_i(X, mu1, mu2, i, sigma)\n","                ## -- ! code required\n","                # update of mu1\n","                mu1 = 0\n","                ## -- ! code required\n","                # update of mu2\n","                mu2 = 0\n","\n","                loss_i = loss(X, mu1, mu2, sigma)\n","                losses.append(loss_i)\n","\n","        plt.plot(losses)\n","        plt.title('ADAM, learning rate = ' + str(learning_rate))\n","        plt.show()\n","        \n","def main(n, d, sigma, n_steps):\n","    np.random.seed(0)\n","    X = generate_data(n, d, sigma)\n","    mu1_init = np.random.rand(d,1)\n","    mu2_init = np.random.rand(d,1)\n","    m_init =  np.zeros((d,1))\n","    v_init =  np.zeros((d,1))\n","    m_init2 =  np.zeros((d,1))\n","    v_init2 =  np.zeros((d,1))\n","    Adam_plot(mu1_init, mu2_init, X, sigma, n_steps, [1e-1, 1e-3, 1e-6],m_init,v_init,m_init2,v_init2)\n","    \n","main(100,4,1,100)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CmFI-LteT4J9"},"source":["**`[double click here to add a Description]`**"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"k-LGsmnET4KB"},"source":["### **Q3.4 (bonus)**: There is another algorithm for fitting the mixture models named [EM](http://cs229.stanford.edu/notes/cs229-notes8.pdf) (Expectation maximization). Complete the EM code given bellow which maximizes the data log-likelihood for the model in Q2.2 and compare it's performance with that of SGD."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"AzhQwxMwT4KD","colab":{}},"source":["def em_plot(mu1_init, mu2_init, X, sigma, n_steps):\n","    mu1 = mu1_init[:]\n","    mu2 = mu2_init[:]\n","    losses = []\n","    for iter in range(n_steps):\n","        # E-step\n","        eta1 = 0 ## -- ! code required\n","        eta2 = 0 ## -- ! code required\n","         \n","        # M-step\n","        mu1 = 0 ## -- ! code required\n","        mu2 = 0 ## -- ! code required\n","        \n","        loss_i = loss(X, mu1, mu2, sigma)\n","        losses.append(loss_i)\n","    plt.plot(losses)\n","    plt.title('EM')\n","    plt.show()\n","\n","def em_main(n, d, sigma, n_steps):\n","    X = generate_data(n, d, sigma)\n","    mu1_init = np.random.rand(d,1)\n","    mu2_init = np.random.rand(d,1)\n","\n","    em_plot(mu1_init, mu2_init, X, sigma, n_steps)\n","    sgd_plot(mu1_init, mu2_init, X, sigma, n_steps, learning_rates = [1e-3])\n","    \n","em_main(100,4,1,100)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-rozDTqlT4KL"},"source":["### Problem 4: Multiclass Logistic Regression [20 points]\n","\n","In the problem, we want to solve K-class classification problem by logistic regression. Assume that we observe data generated via the following model:\n","\n","- We have the constant data matrix $X \\in \\mathbf R^{n \\times d}$. Each row $\\mathbf{x_i}$ is a d-dimensional data vector.\n","- $\\boldsymbol{{\\Theta}}$ is a $d\\times K$ parameter matrix. Each column $\\boldsymbol{\\theta_k}$ is a d-dimensional parameter vector. \n","- $\\boldsymbol\\varepsilon$ is $n\\times K$ noise matrix. Each element $\\varepsilon_{i,k}$ is iid sampled from a distribution whose pdf is in the form of $\\mathbf f(\\varepsilon_{i,k} ) = \\frac{1}{Z}e^{-\\varepsilon_{i,k}}e^{-e^{-\\varepsilon_{i,k}}}$, where $Z$ is a normlizing constant. \n","- and we observe $(y_1,\\dots, y_n)$ where $ y_i = \\arg\\max_{k\\in\\{1,\\dots,K\\}} [\\mathbf{x_i} {\\boldsymbol\\theta_k} + \\varepsilon_{i,k}]$ - the index with largest score.\n"," \n","\n","Show that given model definition above, the following holds:\n","\n","**Q4.1**: The normalizing constant $Z=1$.\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LcavoS8aIv-e"},"source":["**`[double click here to add a solution]`**"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"IYBtztIIIv-g"},"source":["**Q4.2**: For $\\forall k\\neq j$,and $k,j\\in\\{1,\\dots K\\}$, define a random variable $\\varepsilon_{i,k,j} = \\varepsilon_{i,k} - \\varepsilon_{i,j}$, then $\\varepsilon_{i,k,j}$ follows logistic regression. Namely, the cdf is $\\mathbf{F}(\\varepsilon_{i,k,j}) = \\frac{e^{\\varepsilon_{i,k,j}}}{1+e^{\\varepsilon_{i,k,j}}}$."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zxMoAhqvIv-g"},"source":["**`[double click here to add a solution]`**"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rzvVAD1uIv-i"},"source":["**Q4.3**:For $\\forall k\\in\\{1,\\dots,K\\}$, $P(y_i = k \\ | \\ \\boldsymbol{{\\Theta}}, \\mathbf x_i) = \\frac{e^{\\mathbf{x_i}{\\boldsymbol\\theta_{k}}}}{\\sum_{j\\in\\{1,\\dots,K\\}}e^{\\mathbf{x_i}{\\boldsymbol\\theta_{j}}}}$"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"uWYSd5ulIv-j"},"source":["**`[double click here to add a solution]`**\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rjt-DRK0T4Kc"},"source":["From the results of **Q4.3**, we represent $P(y_i = k \\ | \\ \\boldsymbol{{\\Theta}}, \\mathbf x_i)$ as a [softmax function](https://en.wikipedia.org/wiki/Softmax_function) on dot product of data $X$ and parameter $\\Theta$\n","\n","$\\mathbf 1$ denotes indicator function.\n","\n","**Q4.4**:$\\mathcal L_{MLE}(\\boldsymbol{{\\Theta}}) = \\sum_{i=1}^n \\sum_{k=1}^K \\mathbf 1 \\big[y_i =k\\big] \\log(\\frac{e^{\\mathbf{x_i}{\\boldsymbol\\theta_{k}}}}{\\sum_{j=1}^K e^{\\mathbf{x_i}{\\boldsymbol\\theta_{j}}}}) $"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"opyLyvwUT4Kf"},"source":["**`[double click here to add a solution]`**"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ZUMuVy1tIv-m"},"source":["We will see how we can compute $\\Theta_{MLE}$ using numerical method in Problem 5."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pQtsoMDsT4Kx"},"source":["### Problem 5: Coding Logistic Regression [25 points]\n","In this problem, we will use multiclass logistic regression to classify 10 digits from MNIST dataset. The MNIST database of handwritten digits. Images are in the size of $28\\times 28$ pixels. Training set contains 60,000 images with labels. Testing set contains $20,000$ images with labels. \n","\n","First, we load data from torchvision package and visualize examples from the training set. Then we reshape the data and each row vector is a data sample.\n","\n","You will use **`train_data`**, **`train_targets`**, **`test_data`**, **`test_targets`**  in the later part of coding.\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"y_2EhzzZT4Kz","scrolled":false,"colab":{}},"source":["import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","import numpy as np\n","import copy\n","import random\n","import matplotlib.pyplot as plt\n","\n","transform = transforms.Compose([transforms.ToTensor(),\n","                              transforms.Normalize((0.5,), (0.5,)),\n","                              ])\n","mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n","mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=None)\n","\n","train_data_raw = mnist_trainset.data.numpy()\n","train_data = train_data_raw.reshape((len(train_data_raw),-1))\n","train_targets = mnist_trainset.targets.numpy()\n","test_data_raw = mnist_testset.data.numpy()\n","test_data = test_data_raw.reshape((len(test_data_raw),-1))\n","test_targets = mnist_testset.targets.numpy()\n","\n","fig, ax = plt.subplots(2,5)\n","for i, ax in enumerate(ax.flatten()):\n","    im_idx = np.argwhere(train_targets == i)[0]\n","    plottable_image = np.reshape(train_data_raw[im_idx], (28, 28))\n","    ax.imshow(plottable_image, cmap='gray_r')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"tz_dW2GdT4K6"},"source":["**Q5.1**: We want to use numerical methods to solve $\\Theta_{MLE}$. We need to compute the gradient of the loss with respect to model parameters. We define the softmax function from **Q4.3**, for the $i-th$ data sample that has class label k, $h_{\\theta}(x_i)_k = \\frac{e^{\\mathbf{x_i}{\\boldsymbol\\theta_{k}}}}{\\sum_{j\\in\\{1,\\dots,K\\}}e^{\\mathbf{x_i}{\\boldsymbol\\theta_{j}}}}$. Then we can write the $\\mathcal L_{MLE} = \\sum_{i=1}^n \\sum_{k=1}^K \\mathbf 1 \\big[y_i =k\\big] \\log(h_{\\theta}(x_i)_k)$. So we can write the loss as $L =-\\frac{1}{n} \\sum_{i=1}^n \\sum_{k=1}^K \\mathbf 1 \\big[y_i =k\\big] \\log(h_{\\theta}(x_i)_k)$. \n","\n","Show $\\frac{\\partial L}{\\partial \\theta_{sj}}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(h_{\\theta}\\left(\\mathbf{x}_{i}\\right)_{j}-\\mathbf{1}\\left[y_{i}=j\\right]\\right) x_{i s}$, where $s\\in\\{1,\\dots,d\\}$,$j\\in\\{1,\\dots,K\\}$."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"y5xd9t6WT4K7"},"source":["**`[double click here to add a solution]`**\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"BLz8nd7WIv-v"},"source":["Next, we want to implement the multiple class logistic regression by SGD. If we only apply one data sample $(\\mathbf{x}_i, y_i)$ each iteration, the results from **Q5.1** become $\\frac{\\partial L\\left(h_{\\theta}\\right)}{\\partial \\theta_{s j}} = \\left(h_{\\theta}\\left(\\mathbf{x}_{i}\\right)_{j}-\\mathbf{1}\\left[y_{i}=j\\right]\\right) x_{i s}$. Then the$\\frac{\\partial L\\left(h_{\\theta}\\right)}{\\partial \\theta_{j}} = \\left(h_{\\theta}\\left(\\mathbf{x}_{i}\\right)_{j}-\\mathbf{1}\\left[y_{i}=j\\right]\\right)\\cdot x_{i}^T$, this is namely colomn $j$ of $\\frac{\\partial L\\left(h_{\\theta}\\right)}{\\partial \\theta}$. Then you can represent $\\frac{\\partial L\\left(h_{\\theta}\\right)}{\\partial \\theta}$ as a matrix product to reduce the number of loops in the code. This will make our implementation easier."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UzYVB9UXIv-w"},"source":["**Q5.2**: Write (by filling in the missing code below) Logistic Regression Class below. We already have the softmax function implemented. In the train part, apply a iterative numerical method. Report the training and testing accuracy.\n","\n","We marked places where you are expected to add/change your own code with **`## -- ! code required`** comment."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MwZm3mocIv-x","colab":{}},"source":["class LogisticRegression(object):\n","    \"\"\" Multinomial Linear Regression\n","\n","    @attrs:\n","        weights: a parameter of the model\n","        alpha: the step size in gradient descent\n","        n_features: the number of features\n","        n_classes: the number of classes\n","    \"\"\"\n","    def __init__(self, n_features, n_classes):\n","        \"\"\" Initializes a LogisticRegression classifer. \"\"\"\n","        self.learning_rate = 0.01  # tune this parameter\n","        self.n_features = n_features\n","        self.n_classes = n_classes\n","        self.weights = np.zeros((n_features, n_classes))\n","\n","    def train(self, data, labels):\n","        \"\"\" Trains the model, using stochastic gradient descent\n","\n","        @params:\n","            data: an numpy array of data matrix\n","            labels: an numpy array of labels \n","        @return:\n","            None\n","        \"\"\"\n","        ## -- ! code required\n","        \n","\n","    def predict(self, data):\n","        \"\"\" Compute predictions based on the learned parameters\n","\n","        @params:\n","             data: an numpy array of data matrix\n","        @return:\n","            a numpy array of predictions\n","        \"\"\"\n","        ## -- ! code required\n","        pred = 0\n","        return pred\n","    def accuracy(self, data, labels):\n","        \"\"\" Outputs the accuracy of the trained model on a given dataset (data).\n","\n","        @params:\n","            data: a dataset to test the accuracy of the model.\n","            a namedtuple with two fields: inputs and labels\n","        @return:\n","            a float number indicating accuracy (between 0 and 1)\n","        \"\"\"\n","        ## -- ! code required\n","        acc = 0\n","        return acc\n","    def _softmax(self, x):\n","        \"\"\" apply softmax to an array\n","\n","        @params:\n","            x: the original array\n","        @return:\n","            an array with softmax applied elementwise.\n","        \"\"\"\n","        e = np.exp(x - np.max(x))\n","        return e / np.sum(e)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"uOgWe22xIv-2","colab":{}},"source":["feature_dim = train_data.shape[1]\n","cls_num = 10\n","model = LogisticRegression(feature_dim,cls_num)\n","model.train(train_data, train_targets)\n","train_acc = model.accuracy(train_data, train_targets)\n","test_acc = model.accuracy(test_data,test_targets)\n","print('train acc', train_acc, 'test acc', test_acc)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"l3qNepnuT4K8"},"source":["**Q5.2.1**: Why did we implement $softmax(\\mathbf{x})_i = \\frac{e^{x_{i}}}{\\sum_{j=1}^{K} e^{x_{j}}}$ as shown above? What is numerical under- and overflow and how people deal with it?"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wvMxUL8p8GKg"},"source":["**`[double click here to add a solution]`**"]}]}